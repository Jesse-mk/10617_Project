{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_detection_transfer_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8klcDIcZukRD"
      },
      "source": [
        "Transfer Learning Using https://towardsdatascience.com/building-your-own-object-detector-pytorch-vs-tensorflow-and-how-to-even-get-started-1d314691d4ae tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otOs-Bqvv97F",
        "outputId": "6d0b7b81-e398-4189-8104-402b34934f42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/pytorch/vision.git\n",
        "!cd vision\n",
        "!git checkout v0.3.0"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXsPg6Qkrab4"
      },
      "source": [
        "import pycocotools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from PIL import Image\n",
        "import utils as until\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import math\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import pickle\n",
        "\n",
        "# from torch.vision import enginetrain\n",
        "from engine import train_one_epoch, evaluate"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLrbsJvAD77"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctiNc45Ku7xv"
      },
      "source": [
        "import re\n",
        "cv2.startWindowThread()\n",
        "\n",
        "class ExpressionImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    An expression-level dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, pickle_file, transform=None, colab=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pickle_file (string): Path to dataset pickle file.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                    on a sample.\n",
        "        \"\"\"\n",
        "        with open(pickle_file, 'rb') as f:\n",
        "            self.df_data = pd.DataFrame(pickle.load(f))\n",
        "            if colab:\n",
        "                self.df_data[\"img_path\"] = self.df_data[\"img_path\"].apply(lambda x:\"\".join(x.split(\"all_years\")[1:]))\n",
        "                self.df_data[\"img_path\"] = self.df_data[\"img_path\"].apply(lambda x: \"/content/drive/My Drive\"  +  re.sub(r'\\\\', \"/\", x))#/10617 Data\n",
        "                \n",
        "        # print(self.df_data['img_path'].iloc[0])\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df_data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "            \n",
        "        row = self.df_data.iloc[idx]\n",
        "                \n",
        "        traces_data = row['traces_data']\n",
        "        img_path = row['img_path']\n",
        "        tokens = row['tokens']\n",
        "        latex = row['latex']\n",
        "        \n",
        "        # CV2 will read the image with white being 255 and black being 0, but since\n",
        "        # our token-level training set uses binary arrays to represent images, we\n",
        "        # need to binarize our image here as well.\n",
        "        image_raw = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        image_binarized = cv2.threshold(image_raw, 127, 255, cv2.THRESH_BINARY)[1]\n",
        "        image_bitmap = image_binarized / 255.0 #change to be 1's and 0's\n",
        "        \n",
        "        sample = {\n",
        "            'image': image_binarized,\n",
        "            'image_bitmap': image_bitmap,\n",
        "            'traces_data': traces_data,\n",
        "            'tokens': tokens,\n",
        "            'latex': latex\n",
        "        }\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YefeMGO2yPWq"
      },
      "source": [
        "train_exp_path = \"/content/drive/My Drive/train/Copy of train.pickle\" #10617 Data\n",
        "test_exp_path = '/content/drive/My Drive/test/Copy of test.pickle' #10617 Data/\n",
        "\n",
        "# print('train')\n",
        "train_exp_set = ExpressionImageDataset(train_exp_path)\n",
        "# print('test')\n",
        "test_exp_set = ExpressionImageDataset(test_exp_path)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ci59aTT7yhM6"
      },
      "source": [
        "sample = train_exp_set.__getitem__(0)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5Ea0rTYw1Qn",
        "outputId": "f2d6c24f-14da-447e-d192-e6efae095a21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_traces_data = train_exp_set[2]['traces_data']\n",
        "\n",
        "def get_traces_data_stats(traces_data):\n",
        "    all_coords = []\n",
        "    for pattern in traces_data:\n",
        "        for trace in pattern['trace_group']:\n",
        "            all_coords.extend(trace)\n",
        "        \n",
        "    all_coords = np.array(all_coords)\n",
        "    \n",
        "    x_min, y_min = np.min(all_coords, axis=0)\n",
        "    width, height = np.max(all_coords, axis=0) - [x_min, y_min] + 1\n",
        "    \n",
        "    return x_min, y_min, width, height\n",
        "\n",
        "def get_trace_group_bounding_box(trace_group):\n",
        "    all_coords = []\n",
        "    for t in trace_group:\n",
        "        all_coords.extend(t)\n",
        "        \n",
        "    all_coords = np.array(all_coords)\n",
        "    \n",
        "    x_min, y_min = np.min(all_coords, axis=0)\n",
        "    width, height = np.max(all_coords, axis=0) - [x_min, y_min] + 1\n",
        "    \n",
        "    return x_min, y_min, width, height\n",
        "    \n",
        "\n",
        "def draw_traces_data(traces_data):\n",
        "    im_x_min, im_y_min, width, height = get_traces_data_stats(traces_data)\n",
        "    \n",
        "    # Scale the image down.\n",
        "    max_dim = 1000 # Maximum dimension pre-pad.\n",
        "    sf = 1000 / max(height, width)\n",
        "    scaled_height = int(height * sf)\n",
        "    scaled_width = int(width * sf)\n",
        "    \n",
        "    image = np.ones((scaled_height, scaled_width))\n",
        "    \n",
        "    # Draw the traces on the unscaled image.\n",
        "    for pattern in traces_data:\n",
        "        for trace in pattern['trace_group']:\n",
        "            trace = np.array(trace)\n",
        "            trace -= np.array([im_x_min, im_y_min])\n",
        "            trace = (trace.astype(np.float64) * sf).astype(int)\n",
        "            \n",
        "            for coord_idx in range(1, len(trace)):\n",
        "                cv2.line(image, tuple(trace[coord_idx - 1]), tuple(trace[coord_idx]), color=(0), thickness=5)\n",
        "            \n",
        "    # Pad the scaled image.\n",
        "    pad_factor = 0.05\n",
        "    pad_width = ((int(pad_factor * scaled_height), int(pad_factor * scaled_height)), \n",
        "                 (int(pad_factor * scaled_width), int(pad_factor * scaled_width)))\n",
        "    image = np.pad(image, \n",
        "                     pad_width=pad_width, \n",
        "                     mode='constant', \n",
        "                     constant_values=1)\n",
        "    \n",
        "    # Binarize.\n",
        "    image = (image > 0).astype(int) \n",
        "    \n",
        "    # Open CV wants images to be between 0 and 255.\n",
        "    image *= 255\n",
        "    image = image.astype(np.uint8)\n",
        "    \n",
        "    boxes = []\n",
        "    \n",
        "    # Get bounding boxes.\n",
        "    for pattern in traces_data:\n",
        "        trace_group = pattern['trace_group']\n",
        "        rect_x_min, rect_y_min, rect_width, rect_height = get_trace_group_bounding_box(trace_group)\n",
        "        \n",
        "        rect_x_min = (rect_x_min - im_x_min) * sf + pad_width[1][0]\n",
        "        rect_y_min = (rect_y_min - im_y_min) * sf + pad_width[0][0]\n",
        "        \n",
        "        rect_width *= sf\n",
        "        rect_height *= sf\n",
        "        \n",
        "        # Convert bounding box coords to integers.\n",
        "        rect_x_min = int(rect_x_min)\n",
        "        rect_y_min = int(rect_y_min)\n",
        "        rect_width = int(rect_width)\n",
        "        rect_height = int(rect_height)\n",
        "            \n",
        "        boxes.append((rect_x_min, rect_y_min, rect_x_min + rect_width, rect_y_min + rect_height))\n",
        "    \n",
        "    return image, boxes\n",
        "    \n",
        "image, boxes = draw_traces_data(test_traces_data)\n",
        "print(image.shape)\n",
        "print(boxes)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(126, 1100)\n",
            "[(50, 15, 56, 102), (75, 31, 132, 113), (192, 18, 201, 116), (465, 21, 476, 121), (514, 27, 558, 115), (590, 29, 638, 108), (877, 12, 926, 109), (967, 5, 1049, 89), (281, 15, 373, 118), (690, 19, 805, 116)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DD0QxgmI8gTV"
      },
      "source": [
        "true_image = np.array(image, copy=True)\n",
        "for box in true_boxes:\n",
        "    rect_x_min, rect_y_min, rect_width, rect_height = box\n",
        "    true_image = cv2.rectangle(true_image, \n",
        "                  (int(rect_x_min), int(rect_y_min)), \n",
        "                  (int(rect_x_min + rect_width), int(rect_y_min + rect_height)), \n",
        "                  (0), \n",
        "                  5)\n",
        "\n",
        "print('Image with true boxes:')\n",
        "plt.imshow(true_image, cmap='gray')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1yFM97x8Yra"
      },
      "source": [
        "### Making mini Object Recognition Dataset:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "354TeLUD52Ka"
      },
      "source": [
        "Make a Pickle that for objet detection which contains the numpy images (normalized) as well as the predicted boxes.\n",
        "\n",
        "In this case, a smaller dataframe was made just to test the model to see if the flow works. ( ~1000 examples). Other other notebook is run for the entire train/test set to get the entire dataset pickle.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRaEGlzp85EX",
        "outputId": "e104daa9-7c9c-4ec9-c70b-728a0f925b8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "box_list = []\n",
        "numpy_list = []\n",
        "\n",
        "short_len = 700 #not entire dataset\n",
        "\n",
        "for i in range(len(train_exp_set.df_data[:short_len])):\n",
        "    #get the specific row:\n",
        "    curr_row = train_exp_set[i]\n",
        "    test_traces_data = curr_row['traces_data']\n",
        "    #get trace data for row:\n",
        "    image, boxes = draw_traces_data(test_traces_data)\n",
        "\n",
        "    #double check right row:\n",
        "    if str(test_traces_data[0].values()) == str(train_exp_set.df_data.iloc[i][\"traces_data\"][0].values()): #check to make srue traces same:\n",
        "      #append to lists in order to later append to train df\n",
        "      box_list.append(boxes)\n",
        "      numpy_list.append(image)\n",
        "    else: #any errors?\n",
        "      print(\"error at line {}\".format(i))\n",
        "\n",
        "print(len(train_exp_set.df_data), len(box_list)) #shapes ?\n",
        "\n",
        "short_df = train_exp_set.df_data[:short_len].copy() #create a df (short since not entire dataset)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 5.44 s, sys: 356 ms, total: 5.79 s\n",
            "Wall time: 6.77 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yIrOmTgrt9T"
      },
      "source": [
        "### OHE: IS THIS EVEN NEEDED? ###\n",
        "from sklearn.preprocessing import OneHotEncoder as OHE\n",
        "\n",
        "#want OHE labels\n",
        "tokens = train_exp_set.df_data[\"tokens\"].sum()\n",
        "ohe_categories = pd.Series(tokens).unique()\n",
        "ohe_categories, len(ohe_categories)\n",
        "\n",
        "handle = \"ignore\" #or error or ignore... maybe ignore is safer\n",
        "\n",
        "ohe = OHE(categories = [np.array(sorted(ohe_categories))],  handle_unknown=handle)\n",
        "\n",
        "ohe_input = train_exp_set.df_data[\"tokens\"].apply(lambda x: ohe.fit_transform(np.array(x).reshape(-1,1)))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQZeuHceYSBM"
      },
      "source": [
        "#add images, boxes and OHE labels to df\n",
        "short_df[\"true_location\"] = box_list\n",
        "short_df[\"numpy_image\"] = numpy_list\n",
        "short_df[\"labels\"] = ohe_input[:short_len]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUqT5m0DYzRQ"
      },
      "source": [
        "test_split = round(short_len * .8) #split at around 80%\n",
        "\n",
        "#since only using training data, split into two dataframes/pickles for model work.\n",
        "short_df[[\"true_location\", \"numpy_image\", \"img_path\",\"tokens\", \"labels\"]][:test_split].to_pickle(\"train_short_2000_test_df.pkl\")\n",
        "short_df[[\"true_location\", \"numpy_image\", \"img_path\",\"tokens\", \"labels\"]][test_split:].to_pickle(\"test_short_2000_test_df.pkl\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj_JE3e479Fc"
      },
      "source": [
        "### Pytorch object detection tutorial\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i32ugqMS777l"
      },
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, path, data):\n",
        "    self.path = path\n",
        "    self.data = data\n",
        "    pickle_file = os.path.join(self.path, self.data)\n",
        "\n",
        "    with open(pickle_file, 'rb') as f:\n",
        "        self.df_data = pd.DataFrame(pickle.load(f))\n",
        "  def __getitem__(self, index):\n",
        "    row = self.df_data.iloc[index]\n",
        "    images = row[\"numpy_image\"]/255 #normalize images\n",
        "    height = images.shape[0]\n",
        "    width = images.shape[1]\n",
        "    pil_image = Image.fromarray(np.uint8(images))\n",
        "    images = torch.tensor(row[\"numpy_image\"]/255).view(1,height,width).float() #need to reshape to (C, H, W) but 1 channel so 1. also turn to float\n",
        "    images = images.view(1, height, width)\n",
        "\n",
        "    box = np.array(row[\"true_location\"])\n",
        "    # area = (box[:,2] - box[:,0]) * (box[:,3] - box[:,1])\n",
        "    #add buffer of boxes (since some boxes are width or height 0)\n",
        "    box[:,0] -=1\n",
        "    box[:,1] -=1\n",
        "    box[:,2] +=1\n",
        "    box[:,3] +=1\n",
        "\n",
        "    #how many objects?\n",
        "    num_classes = len(box)\n",
        "\n",
        "    img_info = {}\n",
        "    img_info[\"num_classes\"] = torch.tensor(num_classes) #more like how many boxes are present\n",
        "    img_info[\"boxes\"] = torch.tensor(box, dtype=torch.int32) #make boxes integers\n",
        "    img_info[\"image_index\"] = torch.tensor(index) #image number\n",
        "    # img_info[\"box_area\"] =  #do we want this\n",
        "    # img_info[\"tokens\"] = torch.tensor(row[\"tokens\"])\n",
        "    img_info[\"labels\"] = torch.tensor(row[\"labels\"].toarray().sum(axis=0), dtype=torch.int64) #make labels ints\n",
        "\n",
        "    return images, img_info\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df_data)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw1cSFSSQcOs"
      },
      "source": [
        "import os\n",
        "obj_train_pickle = \"train_short_2000_test_df.pkl\"\n",
        "obj_test_pickle = \"test_short_2000_test_df.pkl\"\n",
        "\n",
        "train_img_data = ImageDataset(\"/content\", \"train_short_2000_test_df.pkl\") \n",
        "test_img_data = ImageDataset(\"/content\", \"test_short_2000_test_df.pkl\") "
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIczJEW3i_q1"
      },
      "source": [
        "### Modeling\n",
        "using pretrained resnet 50\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT_lj5YpaetP"
      },
      "source": [
        "#pretrained model\n",
        "mod = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #want pretrained\n",
        "model = mod.float() #keep float or double consistent"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiqrqFWXgftv"
      },
      "source": [
        "#changing the model:\n",
        "num_classes = 101 #is this the right thing to divide by?\n",
        "\n",
        "#number of input features:\n",
        "input_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "#change last layer\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(input_features, num_classes) #output number of classes based on number of tokens\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyIlEtXmlU71"
      },
      "source": [
        "#put data into dataloaders:\n",
        "batch_size = 4\n",
        "shuffle = False\n",
        "num_workers = 1\n",
        "\n",
        "#preparing data\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_img_data, batch_size = batch_size, shuffle = shuffle, num_workers = num_workers, collate_fn=until.collate_fn #need this to have images of different size\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_img_data, batch_size = batch_size, shuffle = shuffle, num_workers = num_workers, collate_fn=until.collate_fn\n",
        ")"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a21EV5D5mBSt",
        "outputId": "6801f144-f155-4ea7-b577-523af4e4a17e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(train_loader), len(test_loader))\n",
        "\n",
        "#also collect garbage and throw out!\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(63, 25)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w0PxV6y9tpB"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe9eWUG7mHyw"
      },
      "source": [
        "#see if available device (GPU)\n",
        "torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "#move to device\n",
        "model.to(device)\n",
        "\n",
        "#optimizer:\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "#they added in a lr scheduler, should we do that?"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byho-ZWSw35I",
        "outputId": "00a3a803-2cb3-4ed0-bdc9-9e1daee67cf4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#train for certain number of epochs (verbose)\n",
        "epochs = 5\n",
        "model = model.float()\n",
        "train_one_epoch(model, optimizer, train_loader, device=device, epoch=epochs, print_freq=100)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first one\n",
            "Epoch: [5]  [  0/125]  eta: 0:03:04  lr: 0.001000  loss: 7.4827 (7.4827)  loss_classifier: 4.6639 (4.6639)  loss_box_reg: 0.2808 (0.2808)  loss_objectness: 2.2009 (2.2009)  loss_rpn_box_reg: 0.3372 (0.3372)  time: 1.4747  data: 0.1449  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 10/125]  eta: 0:01:57  lr: 0.001000  loss: 1.8763 (6.5987)  loss_classifier: 0.5337 (1.2473)  loss_box_reg: 0.2350 (0.3723)  loss_objectness: 0.5137 (4.3197)  loss_rpn_box_reg: 0.2839 (0.6595)  time: 1.0222  data: 0.0224  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 20/125]  eta: 0:01:52  lr: 0.001000  loss: 1.2225 (3.9074)  loss_classifier: 0.3373 (0.7752)  loss_box_reg: 0.1407 (0.2661)  loss_objectness: 0.3354 (2.4004)  loss_rpn_box_reg: 0.2402 (0.4657)  time: 1.0463  data: 0.0103  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 30/125]  eta: 0:01:43  lr: 0.001000  loss: 0.8029 (2.9165)  loss_classifier: 0.1339 (0.5605)  loss_box_reg: 0.0850 (0.2061)  loss_objectness: 0.2508 (1.7189)  loss_rpn_box_reg: 0.2678 (0.4310)  time: 1.1226  data: 0.0107  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 40/125]  eta: 0:01:27  lr: 0.001000  loss: 0.6813 (2.3764)  loss_classifier: 0.1496 (0.4704)  loss_box_reg: 0.0981 (0.1859)  loss_objectness: 0.2317 (1.3474)  loss_rpn_box_reg: 0.2319 (0.3727)  time: 0.9924  data: 0.0104  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 50/125]  eta: 0:01:14  lr: 0.001000  loss: 0.7734 (2.0710)  loss_classifier: 0.1847 (0.4135)  loss_box_reg: 0.1175 (0.1723)  loss_objectness: 0.2053 (1.1392)  loss_rpn_box_reg: 0.1871 (0.3459)  time: 0.8489  data: 0.0100  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 60/125]  eta: 0:01:02  lr: 0.001000  loss: 0.9104 (1.8886)  loss_classifier: 0.1538 (0.3680)  loss_box_reg: 0.0848 (0.1565)  loss_objectness: 0.2744 (0.9994)  loss_rpn_box_reg: 0.3284 (0.3646)  time: 0.8255  data: 0.0104  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 70/125]  eta: 0:00:52  lr: 0.001000  loss: 0.9155 (1.7272)  loss_classifier: 0.1300 (0.3335)  loss_box_reg: 0.0699 (0.1438)  loss_objectness: 0.2441 (0.8920)  loss_rpn_box_reg: 0.3839 (0.3579)  time: 0.8671  data: 0.0104  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 80/125]  eta: 0:00:42  lr: 0.001000  loss: 0.7466 (1.6128)  loss_classifier: 0.1313 (0.3103)  loss_box_reg: 0.0694 (0.1350)  loss_objectness: 0.2098 (0.8074)  loss_rpn_box_reg: 0.3036 (0.3602)  time: 0.8876  data: 0.0101  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [ 90/125]  eta: 0:00:32  lr: 0.001000  loss: 0.8512 (1.5251)  loss_classifier: 0.1685 (0.2962)  loss_box_reg: 0.0871 (0.1314)  loss_objectness: 0.1888 (0.7386)  loss_rpn_box_reg: 0.3855 (0.3589)  time: 0.8826  data: 0.0103  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [100/125]  eta: 0:00:23  lr: 0.001000  loss: 0.8395 (1.4576)  loss_classifier: 0.1890 (0.2871)  loss_box_reg: 0.1148 (0.1314)  loss_objectness: 0.1637 (0.6812)  loss_rpn_box_reg: 0.3563 (0.3580)  time: 0.8936  data: 0.0102  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [110/125]  eta: 0:00:14  lr: 0.001000  loss: 0.8428 (1.4015)  loss_classifier: 0.2106 (0.2808)  loss_box_reg: 0.1472 (0.1334)  loss_objectness: 0.1442 (0.6357)  loss_rpn_box_reg: 0.3010 (0.3516)  time: 0.8967  data: 0.0102  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [120/125]  eta: 0:00:04  lr: 0.001000  loss: 0.8439 (1.3559)  loss_classifier: 0.1960 (0.2750)  loss_box_reg: 0.1408 (0.1347)  loss_objectness: 0.1334 (0.5939)  loss_rpn_box_reg: 0.3018 (0.3524)  time: 0.9937  data: 0.0104  max mem: 13886\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "first one\n",
            "Epoch: [5]  [124/125]  eta: 0:00:00  lr: 0.001000  loss: 0.8438 (1.3384)  loss_classifier: 0.1940 (0.2733)  loss_box_reg: 0.1399 (0.1349)  loss_objectness: 0.1334 (0.5789)  loss_rpn_box_reg: 0.2764 (0.3513)  time: 0.9844  data: 0.0103  max mem: 13886\n",
            "Epoch: [5] Total time: 0:01:57 (0.9422 s / it)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<utils.MetricLogger at 0x7f253534c080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv53pXVC_vik"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-BWGJvOoX1Y"
      },
      "source": [
        "#set model to eval mode:\n",
        "model.eval()\n",
        "\n",
        "#sample to get prediction of\n",
        "index = 3\n",
        "test_img = train_img_data[index][0].to(\"cuda\")\n",
        "test_img\n",
        "\n",
        "#get model predictions\n",
        "with torch.no_grad():\n",
        "  prediction = model([test_img])"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH4kj2HvpRFl",
        "outputId": "d198468e-8655-41db-cedb-aeca0797694d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "#plot prediction over image:\n",
        "test_img = train_img_data[index][0].to(\"cpu\")\n",
        "\n",
        "true_image = test_img.numpy().squeeze(0)\n",
        "\n",
        "# true_image = np.array(image, copy=True)\n",
        "for testbox in prediction[0][\"boxes\"]:\n",
        "\n",
        "    #maybe image off so multiply by scaling factor??\n",
        "    sf = 1 #normal\n",
        "\n",
        "    xmin = testbox[0] * sf\n",
        "    ymin = testbox[1] * sf\n",
        "    xmax = testbox[2] * sf\n",
        "    xmax = testbox[3] * sf\n",
        "\n",
        "    rect_x_min, rect_y_min, rect_width, rect_height = box\n",
        "    true_image = cv2.rectangle(true_image, \n",
        "                  (int(xmin), int(ymin)), \n",
        "                  (int(xmax), int(ymax)), \n",
        "                  (0), \n",
        "                  2)\n",
        "\n",
        "print('Image with true boxes:')\n",
        "plt.figure()\n",
        "plt.imshow(true_image, cmap='gray')\n",
        "plt.show()\n"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image with true boxes:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAABDCAYAAABnXEXxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXgUxfaw35qZZLKREEADhiVsgmwGEhUUQRAQEGRR9gvIKqAIXq+7XuRyUURB2ReFT9n5BQUCJCIiICBCIjshkADZSCDrZF9mqe+PZPpmyEISAoHY7/PMk3RXdXdVV/fpU6dOnRJSSlRUVFRUqheaqi6AioqKikrlowp3FRUVlWqIKtxVVFRUqiGqcFdRUVGphqjCXUVFRaUaogp3FRUVlWrIXRHuQojeQohLQohwIcT7d+MaKioqKiolIyrbz10IoQUuAz2BGCAIGCGlDKnUC6moqKiolMjd0NyfBMKllFellHnAFmDAXbiOioqKikoJ6O7COT2B6ELbMcBTt2YSQkwGJgM4Ozv7tGzZ8i4URUVFRaX68tdffyVKKR8qLu1uCPcyIaVcDawG8PX1lcHBwVVVlErHZDJhsViquhgqKioVZPXq1fj5+eHp6cmCBQuoXbt2VRepWPR6fWRJaXdDuF8HGhTarl+w729Bbm4uEydO5OrVqwghis1jNpvJysq6xyVTUVEpK4mJicTGxgJw7NgxXF1dq7hE5eduCPcgoLkQojH5Qn04MPIuXOe+JC8vj9DQUFatWoW7u3uxeaKiopgyZQpvvfUWWq32HpdQReX+4Pr168ydO5dWrVrx8ssvU7duXUUhMpvNGAwGrA4fFouFc+fOkZmZCUBqaiq///47leUQ8uSTTzJp0iQbhezYsWOsXbuWyMhI3nnnHR599NFKuVZlMnHixBLTKl24SylNQog3gL2AFlgrpbxQ2de5n9FqtTRq1KjErpzFYsHDw4Nx48ZhZ2d3j0uncr+TnZ1NfHw8AHq9Hg8PjxJ7gQ8ye/fuRUrJmTNniI+Px8fHR1F2cnJyOH36tGLelFKSkpKCyWQq9ZxCiGLvlbu7Oy4uLsq22Wzmxo0byvk8PT0ZN26cjbL14osvcvDgQa5cuUKNGjWYMGHCHde5srmnwh1AShkABNyNc6uolIaUkuvXr+Ph4XFffDhzcnKIiIjA09MTvV5fJD0mJoasrCzy8vI4evQoCQkJBAYGcvnyZQCcnZ1ZuHAhw4cPv9dFv+s8++yzDBs2jI0bNxIbG6uYQazY2dnZCGovLy8cHBwAcHV15amnnkKj+Z/Dn729Pc899xz29vZFrnWrsmU0Gjl37hxTpkzh0qVLdOjQoUgvum7duvz3v/9l9OjRbNmyhRkzZlCjRo1Kqfu9oMoGVFVUKhuTycSCBQtYtGgRAwYMYMmSJeh0VfeIm81mZsyYwcaNG2nQoIEimAoTFRWlmBpyc3OV/U5OTkgpiYuL47vvvuPll1++Lz5WlYmTkxPLli0jKSmJwMBAWrduzfDhw9FqtTg5OdG5c2dF4AohbIS7EKJYIV4erD0FnU5Hp06dis3Tu3dvvLy8uHTpEnv37uWVV165o2veS1ThXkWkpqYSEBDwwNjc8/LyOHDgAHl5eSXm0Wq1dO3aFWdn53tYsv9x7do1Zs2aRW5uLuvXr8fHx4e6detWSVkgvxdx8uRJMjMzCQ0NLdMxGo2GHj160KNHD1JSUvjyyy8JDg5m27ZtD5TWWB66du3KgQMHuHHjBo0aNVLGquLi4mzyRUdHF3d4hbl+/TpXrlzB0dGRuLg4du/eXWy+tm3bEh4ezldffVXsB/p+RRXuVURYWBifffYZd8O/32AwEBAQcFv75N1gzZo1DBgwAEdHx3t6XYvFQmBgILm5uQgh0Ov1HDp0yKbbXhW0atUKgODgYOzs7Ojbty9ubm63Pe78+fNYLBZcXV1JTk5m8eLF9+WAXmXh6+vLkSNHmDFjBr17965wL8VisZRod7+V1NRU5R3ZtWtXqYLbzs6OkydP8v3331eZ8lJeVOFehfzjH/9g+vTplX7eq1evEhQUVETzKQ6tVluqAPT09MTJyQkPDw+8vb1LzHf69GkOHDhAkyZN+O677+6561hkZCSBgYE4Ojoyf/58HnvsMbp3735fDESeO3eOXr16cfPmTZo0acKCBQvKVC6z2cyQIUPYvn077du3Z/ny5fegtLdHSsn58+eJi4uzsXFLKSt8v8+fP0+nTp0wmUx88sknPPbYY+U6Pi0tjW+++YaDBw/i5ubG5MmTef7550s13YSGhvLLL7/g5OTEkiVLePjhh4vNl5yczJNPPklERARjx46lf//+5Srb3WTdunUlpv3thXtSUhJHjhzh+eeftxlNf5Bp3Lgxu3bt4rXXXuPkyZNIKXn55ZeLFc4+Pj54eHgUex4hBI0aNcLJyQmtVluqNjVyZL636wsvvFAlPsFxcXFkZGTg5ubGsGHDeOihYiftVSpGoxGdTndbgdamTRsmTJjAZ599xvr16xk1ahQ+Pj63Pb9Wq1V6dkajsVLKfKeYzWZ2797NpEmTSEtLY/ny5YwfP57IyEg+//xzpk6dyuOPP17u8z766KN4e3tz5MgRfv7553IJd5PJxD//+U/Wrl2ruEYGBgbyr3/9i3//+993bJt3d3enTZs2XLlyhWPHjt1Xwr00/vbCffHixXz22Wd8++23vPrqq1VdnEpBCIGPjw/+/v4MHjyY48ePk5yczNSpU+/KTDuLxUJSUhIajYaOHTtW+vnLwh9//EFubi7t2rWjVq1alXru7OxstmzZwrVr10hMTFQESGRkJJ07d+b9998vtfcjhODtt99m7969BAcH8+GHHxaxoUspCQsLIycnhzZt2lS5OakkFi9ezKxZs0hPTwfyhej48ePx8/Nj1apVpKSksHHjxnIPZNvb29O/f3+OHDmCv78/r7/+epmFcnx8PIGBgUgpGThwIAC7d+9m/vz5tG/fnpdffrnY4xITEzGbzbc9vxCCp59+mp07dxIWFlb2SlUx1V64W7uQOp2Oli1bIoQgOTmZffv2YTQa+e233zCZTPj7+1eKZ0VOTg6JiYn4+fmV2BOIj4/HZDIRHBzMhg0b7viapdGrVy/FZDJ69GhFw65MMjIyCAoKQqfTERMTc9frVBybN28G8l9Y6/+VRWhoKPPmzStWEPz555/UqlWrTL0+X19fgoODOXjwICtXrqRevXpKWnR0NAsXLiQvL4/XX39dsdUnJiYCEB4eXiX3tTBGo5FFixYpgh0gIiKCDRs2KDOy9+/fz/fff1+hgcfc3Fx0Oh0nT55k7dq1Ze5JSynx8fFBSkmnTp2oU6cO4eHhnD9/ntWrV5OdnV3scRs2bCAvL4/c3Fy2bdtWao/z1KlTQL53U1W3Q1mp1sJdSsm6det46623sLOzY8+ePfj6+hIQEMA333zDM888o7w8sbGxBAUF3fE18/LyyMjI4OTJkyUOKhoMBiwWCxERETbXTE9PZ/fu3SQkJBR7nKOjI/369bMRCmVhzJgxbN26VfGf7tu3b6XaorOyssjOzsZisXD27NlK92q4Henp6Zw7dw6A2rVrV0o7FuZW/2t7e3tatWrFM888g1ar5eLFi2U6j52dHU2aNOHq1ats3bqVZ555huzsbLZu3UpaWhr29vYIIfj888/p3bs3jz76KCkpKQghOHnyJC1btrxjE8OdEBsbS1RUFC4uLjg6OpKQkIC7u7tyv2vWrElSUhJ+fn4VchQwm83UrFmTxMREAgICaNy4cZmPbdy4MY0bNyYmJob169crCl2tWrVKfB6aN2/O2bNncXJyIjQ0tNR3Ijk5GSEESUlJnDhx4r4Yy7kdD4Rw37VrFxUJLJaamsratWsVTePdd9/l2Wef5cyZM+j1emrWrKm8LA4ODtSsWfOOy2rVPtzc3HBycio2j3XWnZOTk801ExISSEpKKvHcOTk5JCQklHuwydXVlWbNmilueS4uLpXqM63VapWH3cXFpVLuY0Wur9VqqV+/fqVf/9q1azYvc5s2bejWrVuFvCbq1avH1atXSU1Nxc3NDaPRqGiWjzzyCDk5Ody4cQNnZ2dq1qypPLtardbmea0Krl27hpQSjUajuPBan2GLxcJDDz1ESkoKmZmZFWoDs9msnNda//KSlZWlfIwdHByoX79+ie+hlJJ27drh6OhYYqgQKw0bNkSj0dyV9+duUemLdVSE0qJCWiwWevToQYcOHVizZg3e3t4MGTKkSL7U1FRWr17NSy+9xMGDBxk9ejRffvklCQkJ1KtXj9jYWLy8vHj77bdZs2YNFy5cuG8GqVSqln79+tGnT58i+8+ePct3332H2WzGwcGBsWPH0q5duzu6VkJCAp999hk6nY6PP/4YNzc30tLSsLOzw9HRkZ9//pldu3bh6+vLuHHjALh48SJNmzatFMFuNptZvHgx//jHP2wGnU+dOkVYWBhDhw4t9riMjAzmz59PcnIyPXr04MCBA+j1eurXr0/37t1p06YNmzdvVpwTBg8eXK5yrV27lo4dO7Jz505iYmJ45plnymVCvHTpEkuWLCk21kzDhg15++2378jsGhMTw/z583F2dubjjz+udHdIi8XCkiVLGD58eIkODsXx+uuv/yWl9C0u7YHQ3DUaDYMHD2bLli20bduWadOmFckTGxuLv78/AwYMIDIykv79+zN79mxcXFx44403+PDDD6lbty5Tpkzh0KFDjBs3Dg8PD44fP87XX3+NXq9n3rx5XL58mRUrVjBo0CAOHz7MuHHj2L59O+Hh4bRs2ZLIyPwIm/Pnz2fjxo38+eefxZZZCFHkQSus/VnTSstXOM/tKIsbWuHrlJS3LGUu6/7Cabe7H2Ut3+3KXNZ7VZiIiAgOHz5cJN+ff/6p2Nk1Gg2JiYnF5rvdtQqXKSMjA4vFgtFo5Pjx40VMd9euXQPyPwKFr2U1H94pFouFtLQ0goODbWza0dHR3Lx5s8T6paenYzAY0Gq1REdHYzKZqFGjBllZWYSEhJCSksKNGzeA/MlBZblPhe9NQkIC58+fJyMjA8j3firPvbaGayjumYqPj2f//v0lavBlwRrELC8vj2PHjhUbSuJOkFJiMBgICgoq0zyIsvBACPeKcOLECTIyMvDx8SEqKgqAJ554Qun2eXt706VLF3Q6HV9//TVarZa+fftSq1YtVqxYQevWrTl58iSdOnXi0KFDADz88MPExsYipaRPnz60bduWF154gdatW+Pk5MSRI0cAmDlzJq1bt2b69Onk5OQghGDu3Lk2LmLh4eGsX7+eWbNmsXTpUvbu3QvkT3eeOnUqOp2OQ4cOERoaymuvvVZqXdPS0vj000+ZO3duqZOHvvvuO7Zv306/fv2YOnVqkfSLFy/ywQcfKD2acePGKdOtk5OTmTZtGunp6UydOpV+/fopx61atQp/f3+aNWvGwoULbWbd/vHHH6xdu5alS5fy/fff06pVKxo1asTq1auZM2eOjVfI1q1bWbduHa+88oqitZaF+fPnc+jQIUaPHl2mGCyBgYEsW7ZMEfJGo5GRI0falDstLY1ff/1V2baaI1588cUye+P89NNPrF27lo8++kiZ3m42mwkPDyc0NJSWLVvSpUsXJb/FYuGdd94BYPjw4TZplYXJZCIkJIQBAwbwyCOPKPuPHDnCuXPnGD16NJB/T3744QfCwsLo378/V65cUWzi165dw8nJiS+++IKtW7fSpUsXfH19SU9PJzw8nMcee0w5T0n8+uuvLFq0iNmzZ9OhQwdiY2Pp3r070dHRGAwGsrOzGTJkyB3PCP31119ZuXIlP//8M/PmzaNFixYVOs+2bduwWCy4u7szYsSIStfczWYzoaGhDBw4kPr165f5uC1btpSYVm2Fu1XTadOmDefPn0cIQbNmzYp82du1a4erq2uR2Zy///47BoOBd999V5kMdOrUKbKyshS/cSklRqORK1eu2AipHTt28NtvvylT9aWU/PDDDzZf5MzMTGJiYpg9e7aidUD+ZKC5c+cC+T74GRkZikZUEnl5ecTExPD555+XGs7A+pE7ceKEEnWwMMnJyTamqsOHD3PhwgXlGlbb8L59+wgJ+d+SuImJiej1egwGA1999ZXNPU5MTMRgMPD1119z7do1rly5gpOTE1evXmXevHk2XeWbN28CsH//fmJiYpSxCY1GU6qmZB3QPHz4sFLHksjNzeXixYtIKbGzs8NoNBIbG1uk3FZh7ujoqNR927ZtBAUF4eHhgV6vL7WXYDabuXz5MlJK1q9fr2ihUkrFJrxz504OHDigHGOxWIiJibFJu13dS8NsNivPoPU8Ukqio6P59ttvbRSBhIQEDAYD8+fPB/J7GGfOnMFkMin1gHwNFvIH99etW0dYWBg3b97kt99+U9wEg4KCSE5OLrVsly5dwmKxsGbNGn799VdCQ0NJTk5WnsuUlBRF6boTzGYzGo2G7Oxs1qxZQ506dSp0noiICCD/+Vm6dGmlD6hKKYmMjOTbb7+ttA/HAy/cs7Oz2bt3L/7+/mRkZCjBl6wvk9ls5syZM9SoUYMePXoUOb5GjRrodDob4a7T6Xj66afJycnB29ubvXv3kpmZSe3atZWXpUOHDuzZsweLxUJqaqrNOa0PQmE6depEkyZNlO3Y2Fh+++03+vfvT0ZGBps2beL69evcuHHDRpgLIfD19aVHjx4lPuiZmZmsXLmSYcOG4e/vz5EjR5g0aRJt27a1ybd7925u3LhBkyZNirVnnjp1ivDwcCB/EHb8+PFK1z03N5eFCxcSFxfHyJEjixyfnJyMm5tbkTLu27ePL7/8ksWLF7NgwQJ8fHxo3rw5L7/8MikpKXh6etKsWTNatWpFcHAwoaGhSCmJiopShGCrVq3YuHFjsb0So9HIlClTSEhIYMKECcWOx1iJiIhg0qRJpKam0q1bN3r16sUHH3xAkyZNWLZsWYmDZBcvXuTYsWMEBAQQEhJCbGwsCxYsoGfPniVea+vWrRw9ehQADw8PFi9ejF6vx2w2M2rUKCWc7cmTJ4soFtbn8cSJEzRr1oytW7fe9oWXUpKenk5YWBjJyckkJyezfft2Tp8+DeTbnf38/HBycmLcuHHMmTOHRo0aKcf//PPPBAUF8cknn5CXl8f48eMxmUw0aNCAtLQ00tLSbK7XoEEDli9fzr///W/69OlD165dmT17Nps3b6Znz5689957JZY1Li5O6RH26dOHGTNm8NZbbzFo0CAWLlxIaGgoDz30EIsXL74jUwrkP7fDhg0jNDSUKVOmlNpmJWGxWBgzZgyRkZGMGzeOSZMmlet4KSUnTpwgISGBPn36FPsem81mJkyYwL///W8bOXE7SvNKeuCF+5w5c/jqq6+U2YIzZ85U3KIgvzuVk5NDly5daNasWZHjHRwcqFWrFtevX1cEt1arZcSIEcTFxfHiiy9y/vx5pJQkJSVhMpmQUrJnz55itd+S2L17t40GZjQaSUtLY/Xq1QBFPhBWpJQcOHAAe3v7EruoVvfL48ePK72A48ePK/ZLK9bog1euXOHo0aNFJsoUdmG0s7PjzJkzxU6mCQgIKPNkjujoaJKTk5k3bx6nT58mIiKCmjVrKuaBkJAQ9u3bZ2OTNxgMioYI+T0Yq0Z5K0ajUfFBPnLkiE2P4lbOnTtHdHS0onXt2bMHyB8smzt37m21sUaNGnHt2jUyMzNZtGgRf/zxR6nXstbn9OnTfPzxx8qH0qqdF+6xFcZkMikf2YiICGbPnl0m4X7y5EliYmKUHlbhsYWMjAwWLFiARqMhKiqKpUuXFrG5x8fHM3v2bPLy8pTebsOGDTGbzQQHB9t8hOLj45k7dy6nT5/GYDBw8OBBxeXw2LFjzJ49u8SyJiQkKJr94cOHMRgMhISEkJ2drfTeUlJSmDdv3h1r7llZWco4xtatW0tts9Kw9giPHj1axDX2dpjNZn799VeMRiMnTpwosU5RUVGEhobazCO4Ex5o4W6xWDh8+DBGo5GWLVsSFhbG5cuXiY6OVjT4nJwctFotQ4YMKdbboGbNmjRv3pzw8PASB0r0ej3Tp09nx44dnD17FiklU6dOZfny5SQmJtKhQwccHR1JT09HCIG7uzt169bFz89PeSE6depkY0tLSEjgxIkTt/U5v3LlCvv27WP79u20aNGC7t27F8mTnZ2NVqvFaDQq5qioqCieeuopG7OHVfDZ2dlhZ2fHwYMHefjhh/H19UWr1WIymRSt0dnZGXd3d5vBXatmWx630eTkZDQaDa6urtjb2+Pk5ISLiwuPPPII/fr1QwhBWloaQUFB5OXlERUVhVarZejQoWUKY2Ad0xBCUKNGjWLLlZOTw08//URqaioeHh689NJL6HQ6MjMzCQoKIjU1FSGEzbFSSjIyMnB0dFTuYc2aNenRowc7d+4kISEBZ2fnYrV9i8WifPiFEOTl5REcHMyAAQPIzc3FZDKh1+sZNmwYOTk5yn0ODAwkMjKSFi1a0K1bt3J1/a9evUp0dDT29vY88sgj6HQ6nnrqqSKar8ViQafTFXFZTUpKwt7enpo1axIREUFOTg61a9emdevWnD9/HpPJhEajoVu3boSFhREVFcXWrVuB/Jm6AwcOpEWLFoSHhxMfH0+NGjWKFWLZ2dkcP35c+fBER0fzxBNPYGdnh7Ozs6JM1K9f/7azqS0WC3v27CEmJobWrVvTuXNnG2VESsnFixfJzc3Fy8uLNm3aVMiccuPGDeX+NGvWrNzROf/44w+Sk5NxdXWlRo0aihyKiori9OnT9O/fHyEEAwYMICwsrNJmwT7Qwt3Kww8/zPLly5k5cybNmzfnxx9/5ODBg0p6t27dSg0t0Lx5cwIDA0lMTMTT0xMpJbm5uYrN0mr7tfpTWywW2rdvz5w5c5g5cyZ2dnY4ODjQqFEjdDodXl5etG/fnh07dmAymRBC0LdvX5588knlmiEhIcTFxTFx4sRSH7i0tDSSkpL466+/yM3NZcCAAUXipqSkpBAUFISrq6vy0tjb2/PKK68oNkaz2czVq1c5c+YMbdq0ISYmhps3b5KamsqYMWN49tlnWbRokfIxql+/vk3ZpJScO3eOqKgoOnfuXKr5ozC//fYbISEhjB07lvT0dLy9vWnatCkRERGMHz8erVaL2WxWfPjHjh2L2Wxm6NChZRpYSk9PZ8+ePcq9sc7stCKlJDAwUNFmX331VYYMGYIQgoSEBHbs2EFeXh5Dhw6lYcOGynERERHMmTOHQYMG2QweBwQE4O/vj5ubG2PHji3WbHDz5k38/Pyws7OjU6dO/P777zg5OTFy5EjCw8PZtGkT9vb2jBw5klq1aikC6fz580RGRtK3b19GjRpVLkGUm5tLSEgIer1eMbc4OjoW6XkZjUaCgoIYNmwYDRr8b6njffv2ERERwcKFC/Hz8yMwMJB69eoxdepUBg0aBMDQoUNZtGgR4eHh9OzZU1kHWAjBqFGj0Ol07Nmzh3r16jF//vxiFaXAwEA2btyobOfl5TFp0iQyMjIYMmQIV65cISkpif79+5fYW7NisVgYP348P/zwA9evX6dOnTo2C2ocPnyY7du3o9FoePfdd4t1IigL69evZ/v27TRt2pQlS5aUy/8+NTWVXr16Afn33sfHh5EjRxIVFcXkyZNp2rQpy5Ytq3C4iRUrVpSY9kD4uffq1Yv//Oc/DB06lMGDB7N48WIlrWvXrrRv3553332XIUOG8N///pfBgweTlZWFyWTCwcGB/fv38/TTTwP5Xd4RI0Ywffp0xRthxYoVTJs2jT59+jBt2jSGDBnCo48+SlxcHLVq1VL+pqSkkJ2djdFoxN3dXVn6y7pijFVTsQbZSklJUepxa/RFKSVms7lMvrdms9nmA3Prg2A9161tWTiolZRSEdy3uiVaJ6VYTU7WPLeWzZp+u0iShbFYLEo9C4djFUIU6/JlMBgwmUw4OjqWeWApLS2NvLw8XFxcijVd5eXlkZeXh16vt7knFouFlJQUpJS4u7vbaJqZmZlkZ2djZ2eHm5ubMqhqvc+urq7Fau1Go5HMzEylDo6OjmRlZZGTk4ODgwMmk6nE+lnrodfr72rs9tTUVFxcXGzqaw0vXLduXSIiIrhx4wZ6vZ6HH36Y6OhonJ2dadq0KU5OTuTk5HDhwgWcnJzIyspSNFqrqc3e3p62bdsWeX6MRqNi/rGOn2RnZ9OgQQOysrKws7MjPj6ehg0bEhAQUKbJejdv3mTUqFHs378fIQQNGjRQPrg3btzAYDDQuXNn/P39bztRqSTeeecdvvrqK3r27MnevXvL7G577Ngx5s2bx+7du9FoNFgsFjQaDV5eXiQlJWEwGBg6dChbtmyp8ACtEOLB9nO/HU2aNFGEjRACo9GoCEOz2cy5c+fw8fHhyJEjLFiwgJCQEJKSkpSBS+tgUU5ODu3atcPb25s333yTjRs30r17dzZt2kSXLl0IDQ0lPj6eixcv8v3332M0Ghk1ahTe3t44OztTt25dtFotXl5eeHt7M2rUKEVjdHV1LWJzz8jIUB64vLw8xQ5Zq1YtGxNS4TQ3N7ci5iWLxaIEtLJ2h61C0moOsc581ev15ObmYmdnp7j03eolY50Gby2b0WhEo9GQmppKXl4ezs7OZR7oysnJIT09ndq1a5Oenq5M1unWrRvz5s0r8sFbtGgRCxYs4PHHH2fTpk1lmgn4xhtvsHPnTqZOncqbb75ZJD0rK4ukpCQbTRXy7cbdu3cnNzcXPz8/Gze5devW8dFHH+Hi4sKUKVPYtWsXZ86cQQjByJEjmTNnTrFl27BhAx988AF6vZ6lS5fSq1cvduzYwYwZM7C3t8dkMuHk5MTmzZvp0KGDzbHffvstX331FW+++SbTpk27K1PcrW6fX375JV5eXsXm2b59OzNnziQ3N5fo6GhcXV2VOSaQP2Ywffp0tmzZwvvvv0///v3p3r07mZmZDBw4kISEBBYvXlzkfi9dupQvvviCGjVqsG3bNq5fv86kSZPIzc3loYceIjQ0lLp167Jly5Yyz8L28PBg48aNLFq0iLVr19p4WTk4OPDcc8+xbt26Cgt2KaWy0EqXLl1u2yaxsR68564AAA/FSURBVLH8+eefBAUFsXLlSgwGA/b29owdO5ZHHnmE5cuXc/XqVRwcHOjWrVuZwz9XuPBV/fPx8ZElYTab5fPPPy+PHj0qPT095fTp023SOnfuLBcuXCivX78uO3ToICdNmiSFENLe3l6+8MIL0sHBQdrb28uXXnpJenh4SK1WK319faW3t7d88skn5ZNPPim9vLwkIN3c3KSvr690c3OTbdq0kXXq1JHNmjWTNWrUkA0bNpS1a9eWNWrUkBqNRj7++OOyXbt2UqPRSDc3N1mrVi1Zt25dWa9ePenl5aWkubq6ygYNGsjt27fLpKQk5bd//37Zs2dPmZCQIJOSkuTFixdlvXr1pBBCLly4UMbHxyt5/fz8lOscP37c5jyJiYly8eLF0s7OTrq4uMh9+/bJM2fOyCZNmkhADhw4UI4bN05qNBqp1+vluHHjJCB9fX3lzZs35Y4dO6S7u7sElJ+1/n/88Yc8ceKE7Ny5s5w4caLs16+fBOT8+fNtylDa74cffpBeXl4yKipKTpw4UX733XcyKSlJZmdnF9vekZGR8sUXX5Tz5s2TFoulxOeiMCNGjJCA/OCDD8qU34rBYJAtWrSQgFyzZo1N2rVr12S7du1s7ouDg4McM2aMTE1NLfGcx48fl4MGDZKrVq2SJpNJSillSkqKzblmzJghjUZjkWOzs7PlX3/9JbOysspVj/KQl5cnu3btKsPCwkrMk5WVJT/55BPZokUL2bt3b7lt2zabtoiMjJSdO3eWOTk5cuzYsTIgIEBJO3r0qNy1a5c0m80258zMzJQ+Pj5SCCHnzJkjzWazTE9Pl0899ZTN/V2yZEmF63b58mX5yy+/yD179sg9e/bIP//8U2ZmZlb4fFJKmZSUJD09PaVGo5FbtmwpNW9ubq7s1auXUh8hhGzZsqX09/eXOTk50mw2y6ioKBkQECCPHz9+x2WTUkogWJYgV6uF5r5p0yblC3vq1Cl0Oh0fffQR7777LitXruSjjz7C398frVbL9OnTmTNnjs3xW7duZdKkSfj6+rJp0yaGDx/Op59+yooVK3jxxRdZtWoVffr04fTp08TGxnLmzBlFc3/uuedwd3fHwcEBV1dXNBoNtWvXpkGDBly8eJE6derg5OSEv7+/jSdHTEwMUVFRrFy5Eo1Gg9lsxtnZGSkln376KRcuXKBJkyZYLBb8/PyULl1AQICN5m42m1m7di1Go5EWLVpw7NgxtFqtEsNkx44dSt7mzZsrXig3b95UJhYVNh9Bfrc9NTWVvn37KgNsMTEximb7448/ljl0Q0hICGlpaaxZs0aZgWj1iCiJJ554AqPRyOeff16ma1gHkTdv3oyTk1O57JfWGaj+/v5F5hN4e3srKyJB/qB448aNWbp0aannbN++PYmJiXzxxRfKvqZNm3L27Fll0LA0e/LPP/9c5vKXF7PZTGRkJMuXLy/V59ve3p7Bgwej1+u5dOmSTVsYDAaio6P54osvOHPmDPC/qIlWzp49a7NtsVjIzs5GCEF4eDjz5s0D8j2QTpw4gZQSLy8vkpOT+eyzzyqruuzfv/+Ojs/KylIG3I8ePcqVK1dKzGuxWLh69aqy7enpSe/evTl37pwS2K4whSfJ3Q0eeJu7Nc6F1Y6s0+l49tln2bt3L3Z2dpjNZo4ePcry5cvp2LEjU6ZMKWKXvXTpEu3bt1cWRwgLC8PLy4vY2Fhq165NbGwstWrVIiMjg7y8PDIzM3n88cexWCycO3cOR0dHcnJyyr2snVarLdaunJmZWSS8rIODQ4mxRaSUZGVl4eTkVKYunnU8woqTk5ONfdRsNiveRhaLxcZlLjMzEyFEmc0yRqMRT09PJk+ejL+/P40bNy51RaeKkJqaypdffonJZOKtt94qV2yOvxtms5lVq1YxdOjQCk/oSUlJYcOGDUydOhU/Pz+8vb3LHczuVtavX0/Hjh1p3rz5HZ2nstm8eTPBwcG0bt2aV1999b6Ls//2229X3OYuhFgL9APipZRtCvbVArYCXkAEMFRKmSLyJcsioC+QBbwqpTxZGZUQQmBnZ8e+ffsYO3asst86e9T6kXJ1dSUsLIyJEyfaHG9dA7G4qfzWgSyz2UxWVhYajQY7OztlZXSNRoNOp1MGEvV6PbNmzcJkMjFhwgS+/fZbDh8+zLZt2xBC4OjoiJubG6dOncLV1RWdToeHh0eRUfYuXboongiFOXDgANu3b+fkyZNotVq6d+/OpEmTbKaLF8ZgMPDWW2+xbNmyMgnd7du3s2TJEiB/ceKZM2faDG5euHCBNWvWMH78eHJycvDx8VE+nocOHcLBwaHMi3IcPHiQc+fOMWbMGC5evEjHjh0ZMGBAmY4tK1lZWfz444+EhITQoEGDMnvy/B0xGo389NNPvPTSS+UKqVuYmJgYfvnlF0aMGMFff/1Ft27deP755++oXIcPH2bIkCHFuvpWFZcvX1Z6WOPHj+ef//xnFZeoKG+//XaJabfV3IUQXYAMYF0h4T4fSJZSzhNCvA+4SynfE0L0BaaTL9yfAhZJKZ+6XQHLorl/88037N27l0OHDjF58uQi+ZKTk/n888+ZOXMmS5cuVdy0KkJcXBy1a9fGYDDg7OxMamoqDz30EHZ2dhgMBlJSUqhZsyZSSiV0q9ULwuo1o9PpMBgMiieNg4NDucKEyoJAQlavktI0ciklaWlpuLq6lnlwxjppytnZuVivmKysrEpZLs/qqeLi4kJmZiZ2dnZ3JWxtWloaRqMRZ2fnB2qF+qqgOG+Z8mCxWEhPT8fNzY2MjAz0ev0dh8DNyMhg2rRpd9wDqEyOHTvG999/D+QL0ftxgfLJkydXXHOXUv4uhPC6ZfcA4LmC/38ADgLvFexfV2Do/1MIUVMIUU9KefuVmstA/fr18fLysvE7thIbG8uKFSsYM2YMo0ePVuyk5cVisTBw4EDGjh3L1q1b6dq1K//3f//Hv/71Lxo0aMCWLVtYtmyZjZ268GzKW7Hapq2zXyvC7eJ0QL5979ZwAxXFYDBw5cqVMq3zeTusduzevXuzb98+6tevf1deYLPZzMaNG4mNjcXd3Z2BAwdWmzVxKxOz2cyWLVvo27dvhT1IUlNT2bVrF8OHD1fWO23atGmllK+wzboqsVgsHD16FCkljz/+ODqd7r4pW1mp6ICqRyGBfQOwGjk9gcLL8MQU7Csi3IUQk4HJgM3kkcrgTuJRSCnp0aMHGzZs4MKFC8TFxREZGamYPCIjI2nSpAkNGjQgOzubS5cu0bZtWxITExUh7ODggIuLC6GhoTg7O6PVaqlTp85dFTZarbbIwGhFycjIwGg0Vsr5srKyeOaZZ/jwww9JTk6mU6dOJa5peSfIAjfQn376CZ1Ox/PPP3/HpoLqiNFo5NixY0ydOrVcMUwKEx0dzalTp3jvvfe4fv06o0ePLjYe/oNOnz59+OGHH3jjjTcqRdG5G1gHpoujTAOqBZr77kJmGYOUsmah9BQppbsQYjcwT0p5pGD/fuA9KWWpyyiVZbGOQYMGERkZSWhoaLGBe5KTk/niiy/4z3/+c8exlq2uRIsXL6ZTp07s2rWL1157DU9PT3bu3InFYmHQoEHExcWxfPlyZs2axdGjR1m7di2Qb9/X6/XKRBCNRvNAmQvy8vIUU9Sdkp2drUxlT05ORq/XV3q4VCvWdgOUiVIqRblx4wZ16tSp8OIVJpOJxMREPDw8yMzMZPfu3VW2MPrdRpZhnYSqpLRJTBUV7peA56SUcUKIesBBKWULIcSqgv8335rvNudPBy6Vo07VgTpA5azA8OCg1vnvgVrne0cjKWWxWlhFzTL+wFhgXsHfnYX2vyGE2EL+gGpqGe3tl0r6+lRXhBDBap2rP2qd/x7cj3UuiyvkZvIHT+sIIWKAWeQL9f8TQkwAIgHrwosB5HvKhJPvCln25XRUVFRUVCqNsnjLjCghqchoVYGXzOt3WigVFRUVlTvjfplutbqqC1AFqHX+e6DW+e/BfVfn+yL8gIqKiopK5XK/aO4qKioqKpWIKtxVVFRUqiFVLtyFEL2FEJeEEOEFcWoeeIQQDYQQB4QQIUKIC0KIGQX7awkh9gkhwgr+uhfsF0KIxQX34KwQokPpV7h/EUJohRCnCia0IYRoLIQ4XlC3rUII+4L9+oLt8IJ0r6osd0UpCLGxTQgRKoS4KIToVN3bWQjxVsFzfV4IsVkI4VDd2lkIsVYIES+EOF9oX7nbVQgxtiB/mBBibHHXultUqXAXQmiBZUAfoBUwQgjRqvSjHghMwNtSylZAR+D1gnq9D+yXUjYH9hdsQ379mxf8JgMlL4x4/zMDuFho+wvgayllMyAFmFCwfwKQUrD/64J8DyKLgJ+llC2Bx8mve7VtZyGEJ/Am4FswqVELDKf6tfP3QO9b9pWrXUV+9NxZ5M/5eRKYZf0g3BNKWsXjXvyATsDeQtsfAB9UZZnuUj13Aj3Jn4Vbr2BfPfInbwGsAkYUyq/ke5B+QH3yH/ruwG5AkD9rT3drewN7gU4F/+sK8omqrkM56+sGXLu13NW5nflf/KhaBe22G3ihOrYz+SHNz1e0XYERwKpC+23y3e1fVZtlSgo0Vm0o6Ia2B45T/oBrDxrfAO8C1pCctQGDlNK6Mkjheil1LkhPLcj/INEYSAD+X4Ep6jshhDPVuJ2llNeBr4Ao8gMCpgJ/Ub3b2Up527VK27uqhXu1RgjhAvwIzJRSphVOk/mf8mrjhyqEsC7o8ldVl+UeogM6ACuklO2BTP7XVQeqZTu7kx/auzHwCOBMUfNFtedBaNeqFu7XgcJLpNcv2PfAI4SwI1+wb5RS/lSw+2ZBoDUK/sYX7K8O9+EZ4CUhRASwhXzTzCKgphDCOhO6cL2UOhekuwFJ97LAlUAMECOlPF6wvY18YV+d27kHcE1KmSClNAI/kd/21bmdrZS3Xau0vatauAcBzQtG2u3JH5jxr+Iy3TFCCAGsAS5KKRcWSrIGXIOiAdfGFIy6d6TsAdfuG6SUH0gp60spvchvx9+klKOAA8ArBdlurbP1XrxSkP++1oRuRUp5A4gWQrQo2PU8EEI1bmfyzTEdhRBOBc+5tc7Vtp0LUd523Qv0EkK4F/R4ehXsuzfcB4MWfYHLwBXgo6ouTyXVqTP5XbazwOmCX1/ybY37gTDgV6BWQX5BvtfQFeAc+Z4IVV6PO6j/c+SHiAZoApwgP5icH6Av2O9QsB1ekN6kqstdwbp6A8EFbb0DcK/u7QzMBkKB88B6QF/d2hnYTP6YgpH8HtqEirQrML6g7uHAuHtZBzX8gIqKiko1pKrNMioqKioqdwFVuKuoqKhUQ1ThrqKiolINUYW7ioqKSjVEFe4qKioq1RBVuKuoqKhUQ1ThrqKiolIN+f8uVjJarnewBgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL2qFOG1-fQB"
      },
      "source": [
        "### TODO:\n",
        "* i think the model converts to pil image which may be why bounding boxes are so off... Because boxes are based on numpy not pil image...\n",
        "* drop OHE for labels... How do we also denote there's >1 of a certain symbol??\n",
        "* train for longer on a validation set (didn't use test data at all)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kokUGu7m-EdQ"
      },
      "source": [
        "### Scratch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcBllI6E-BjY"
      },
      "source": [
        "#testing specific inputs with the model\n",
        "targets = [ train_img_data[2][1], train_img_data[1][1] ]\n",
        "images = [ train_img_data[1][0].view(1,70, 1097).float().to(device), new_img.view(1,715, 1100).float().to(device)]\n",
        "images = list(image.to(device) for image in images)\n",
        "targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "m = model.float()\n",
        "m(images, targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyKafRzjZ17J",
        "outputId": "5f9e3eea-8780-45ce-8b03-40a3fd88d141",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "#check to see if pil image will keep the bounding boxes the same... I don't think so.\n",
        "\n",
        "from PIL import ImageDraw\n",
        "# [597.8450317382812, 47.35714340209961, 609.9963989257812, 49.35714340209961] \n",
        "xmin = 596.8450317382812\n",
        "ymin =  46.35714340209961\n",
        "xmax =  610.9963989257812\n",
        "ymax = 48.35714340209961\n",
        "tester = Image.fromarray(image)\n",
        "draw = ImageDraw.Draw(tester)\n",
        "draw.rectangle([(xmin, ymin), (xmax, ymax)], outline ='red')\n",
        "tester"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=1100x138 at 0x7F25C67382E8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cRcfX6v_-Ql"
      },
      "source": [
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
        "    \"\"\"Training loop copied from https://github.com/pytorch/vision/blob/master/references/detection/engine.py \"\"\"\n",
        "    model.train()\n",
        "    metric_logger = until.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter('lr', until.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
        "    header = 'Epoch: [{}]'.format(epoch)\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1. / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = until.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = until.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}